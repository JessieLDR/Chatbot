{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc985cc",
   "metadata": {},
   "source": [
    "# ChatbotÂ : webscraping & human interaction\n",
    "\n",
    "As someone in Data Science with experience in human-in-the-loop and real-time human measures, designing and programming a human interaction chatbot sounded interesting.  Without following a known chatbot structure, I tried to be creative and come up with my own logical structure.  \n",
    "\n",
    "I looked at a few medium blogs, and I tried to first make a \"Self-learning Retrieval Based chatbot\" over a Rule-based chatbot or Self-learning Generative chatbot.\n",
    "\n",
    "Check out github for the supporting python subfunctions : https://github.com/j622amilah/Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147c4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a24878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Personal python functions\n",
    "import sys\n",
    "sys.path.insert(1, 'C:\\\\Users\\\\jamilah\\\\Documents\\\\Subfunctions_python')\n",
    "\n",
    "from findall import *\n",
    "from dictionary.sort_dict_by_value import *\n",
    "from string_text_processing.text_url_2_senANDwordtokens import *\n",
    "from string_text_processing.preprocessing import *\n",
    "from string_text_processing.get_word_count_uniquewords import *\n",
    "from string_text_processing.remove_chars_from_wordtoken import *\n",
    "from string_text_processing.get_cossine_similarity import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f20c3",
   "metadata": {},
   "source": [
    "# Get Knowlege base for chatbot by Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a347707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 209 sentences\n"
     ]
    }
   ],
   "source": [
    "inputurl = \"https://en.wikipedia.org/wiki/Chatbot\"\n",
    "name_txtfile = 'chatbot_wikipedia'\n",
    "sen, word_tok = text_url_2_senANDwordtokens(inputurl, name_txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9000b1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 82 sentences\n"
     ]
    }
   ],
   "source": [
    "inputurl = \"https://en.wikipedia.org/wiki/Q%26A_software\"\n",
    "name_txtfile = 'chatbot_wikipedia'\n",
    "sen, word_tok = text_url_2_senANDwordtokens(inputurl, name_txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7d8b495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1493 sentences\n"
     ]
    }
   ],
   "source": [
    "inputurl = \"https://preply.com/en/blog/22-useful-english-greetings-for-every-day/\"\n",
    "name_txtfile = 'how2sayHello'\n",
    "sen, word_tok = text_url_2_senANDwordtokens(inputurl, name_txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23599999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1591 sentences\n"
     ]
    }
   ],
   "source": [
    "inputurl = \"https://regardsurlefrancais.com/2017/09/19/les-salutations-en-francais/\"\n",
    "name_txtfile = 'commentDitBonjour'\n",
    "sen, word_tok = text_url_2_senANDwordtokens(inputurl, name_txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f6b3ff0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'QampA',\n",
       " 'software',\n",
       " 'is',\n",
       " 'software>online',\n",
       " 'that',\n",
       " 'attempts',\n",
       " 'to',\n",
       " 'answer',\n",
       " 'questions',\n",
       " 'asked',\n",
       " 'by',\n",
       " 'users',\n",
       " 'QampA',\n",
       " 'stands',\n",
       " 'for',\n",
       " 'question',\n",
       " 'and']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bcbae",
   "metadata": {},
   "source": [
    "# Retrieval Based chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3d0b9",
   "metadata": {},
   "source": [
    "# Logic and Parser Rules\n",
    "\n",
    "Give the chatbot text, and it will behave based on the text.\n",
    "If you want it to say 'greetings', give it text on 'greetings' and 'manners'.\n",
    "\n",
    "Need to keep the texts separate maybe, based on text A it would say AAA, \n",
    "and based on text B it would say BBB.  Then decide if AAA or BBB is better, based on similarity maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4d0379c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 921 word tokens, but 337 words are unique.\n",
      "[['47' 'qampa']\n",
      " ['44' 'answer']\n",
      " ['31' 'question']\n",
      " ['18' 'commun']\n",
      " ['15' 'onlin']\n",
      " ['15' 'peopl']\n",
      " ['14' 'softwar']\n",
      " ['14' 'motiv']\n",
      " ['14' 'servic']\n",
      " ['12' 'contribut']\n",
      " ['11' 'knowledg']\n",
      " ['11' 'social']\n",
      " ['10' 'yahoo']\n",
      " ['8' 'provid']\n",
      " ['8' 'inform']\n",
      " ['7' 'behavior']\n",
      " ['7' 'featur']\n",
      " ['7' 'launch']\n",
      " ['6' 'qualiti']\n",
      " ['6' 'expert']\n",
      " ['6' 'software']\n",
      " ['6' 'googl']\n",
      " ['6' 'connect']\n",
      " ['6' 'sites']\n",
      " ['5' 'questions']\n",
      " ['5' 'graph']\n",
      " ['5' 'stack']\n",
      " ['5' 'number']\n",
      " ['5' 'allow']\n",
      " ['4' 'contributor']\n",
      " ['4' 'comparison']\n",
      " ['4' 'factor']\n",
      " ['4' 'specialist']\n",
      " ['4' 'share']\n",
      " ['4' 'support']\n",
      " ['4' 'increas']\n",
      " ['4' 'exchange']\n",
      " ['4' 'knowledge']\n",
      " ['4' 'survey']\n",
      " ['4' 'system']\n",
      " ['4' 'member']\n",
      " ['4' 'differ']\n",
      " ['4' 'websit']\n",
      " ['4' 'point']\n",
      " ['4' 'expect']\n",
      " ['3' 'exampl']\n",
      " ['3' 'exchang']\n",
      " ['3' 'signific']\n",
      " ['3' 'relat']\n",
      " ['3' 'reward']\n",
      " ['3' 'three']\n",
      " ['3' 'involv']\n",
      " ['3' 'network']\n",
      " ['3' 'potenti']\n",
      " ['3' 'similar']\n",
      " ['3' 'includ']\n",
      " ['3' 'particip']\n",
      " ['3' 'trust']\n",
      " ['3' 'decemb']\n",
      " ['3' 'domain']\n",
      " ['3' 'these']\n",
      " ['3' 'activ']\n",
      " ['3' 'answers']\n",
      " ['3' 'communities']\n",
      " ['2' 'learn']\n",
      " ['2' 'intrins']\n",
      " ['2' 'charact']\n",
      " ['2' 'chang']\n",
      " ['2' 'chanc']\n",
      " ['2' 'later']\n",
      " ['2' 'buser']\n",
      " ['2' 'assess']\n",
      " ['2' 'mobil']\n",
      " ['2' 'mechan']\n",
      " ['2' 'asker']\n",
      " ['2' 'interact']\n",
      " ['2' 'model']\n",
      " ['2' 'needs']\n",
      " ['2' 'applications']\n",
      " ['2' 'activity']\n",
      " ['2' 'obtain']\n",
      " ['2' 'other']\n",
      " ['2' 'interest']\n",
      " ['2' 'earli']\n",
      " ['2' 'integr']\n",
      " ['2' 'individu']\n",
      " ['2' 'version']\n",
      " ['2' 'crowdsourc']\n",
      " ['2' 'crowd']\n",
      " ['2' 'countri']\n",
      " ['2' 'control']\n",
      " ['2' 'explain']\n",
      " ['2' 'extrins']\n",
      " ['2' 'contributors']\n",
      " ['2' 'complet']\n",
      " ['2' 'compar']\n",
      " ['2' 'field']\n",
      " ['2' 'found']\n",
      " ['2' 'gener']\n",
      " ['2' 'common']\n",
      " ['2' 'howev']\n",
      " ['2' 'collabor']\n",
      " ['2' 'impact']\n",
      " ['2' 'close']\n",
      " ['2' 'classifi']\n",
      " ['2' 'individual']\n",
      " ['2' 'topic']\n",
      " ['2' 'public']\n",
      " ['2' 'services']\n",
      " ['2' 'research']\n",
      " ['2' 'upvot']\n",
      " ['2' 'verif']\n",
      " ['2' 'quora']\n",
      " ['2' 'replac']\n",
      " ['2' 'specif']\n",
      " ['2' 'receiv']\n",
      " ['2' 'service']\n",
      " ['2' 'systems']\n",
      " ['2' 'recent']\n",
      " ['2' 'platform']\n",
      " ['2' 'regard']\n",
      " ['2' 'sourc']\n",
      " ['1' 'constraints']\n",
      " ['1' 'continu']\n",
      " ['1' 'critic']\n",
      " ['1' 'creation']\n",
      " ['1' 'coupons']\n",
      " ['1' 'staffer']\n",
      " ['1' 'constitut']\n",
      " ['1' 'statist']\n",
      " ['1' 'spatial']\n",
      " ['1' 'corpor']\n",
      " ['1' 'coordin']\n",
      " ['1' 'start']\n",
      " ['1' 'consid']\n",
      " ['1' 'compens']\n",
      " ['1' 'stand']\n",
      " ['1' 'speed']\n",
      " ['1' 'contribute']\n",
      " ['1' 'while']\n",
      " ['1' 'sources']\n",
      " ['1' 'special']\n",
      " ['1' 'place']\n",
      " ['1' 'curios']\n",
      " ['1' 'statu']\n",
      " ['1' 'either']\n",
      " ['1' 'effect']\n",
      " ['1' 'slogan']\n",
      " ['1' 'eager']\n",
      " ['1' 'driven']\n",
      " ['1' 'drive']\n",
      " ['1' 'downvot']\n",
      " ['1' 'smartphon']\n",
      " ['1' 'display']\n",
      " ['1' 'discuss']\n",
      " ['1' 'discontinu']\n",
      " ['1' 'direct']\n",
      " ['1' 'digit']\n",
      " ['1' 'tradit']\n",
      " ['1' 'developers']\n",
      " ['1' 'develop']\n",
      " ['1' 'desir']\n",
      " ['1' 'design']\n",
      " ['1' 'describ']\n",
      " ['1' 'degre']\n",
      " ['1' 'definit']\n",
      " ['1' 'defin']\n",
      " ['1' 'defect']\n",
      " ['1' 'decid']\n",
      " ['1' 'solutions']\n",
      " ['1' 'community']\n",
      " ['1' 'structure']\n",
      " ['1' 'widespread']\n",
      " ['1' 'still']\n",
      " ['1' 'technolog']\n",
      " ['1' 'asian']\n",
      " ['1' 'archiv']\n",
      " ['1' 'april']\n",
      " ['1' 'approv']\n",
      " ['1' 'templ']\n",
      " ['1' 'appeal']\n",
      " ['1' 'tempor']\n",
      " ['1' 'answering']\n",
      " ['1' 'answered']\n",
      " ['1' 'without']\n",
      " ['1' 'among']\n",
      " ['1' 'altern']\n",
      " ['1' 'theme']\n",
      " ['1' 'affect']\n",
      " ['1' 'advantag']\n",
      " ['1' 'adopt']\n",
      " ['1' 'addit']\n",
      " ['1' 'theori']\n",
      " ['1' 'activities']\n",
      " ['1' 'there']\n",
      " ['1' 'accord']\n",
      " ['1' 'access']\n",
      " ['1' 'accept']\n",
      " ['1' 'abund']\n",
      " ['1' 'askers']\n",
      " ['1' 'asking']\n",
      " ['1' 'techniqu']\n",
      " ['1' 'categor']\n",
      " ['1' 'structur']\n",
      " ['1' 'cognit']\n",
      " ['1' 'encourag']\n",
      " ['1' 'structures']\n",
      " ['1' 'china']\n",
      " ['1' 'chiebukuro']\n",
      " ['1' 'check']\n",
      " ['1' 'characterist']\n",
      " ['1' 'suggest']\n",
      " ['1' 'surpass']\n",
      " ['1' 'categories']\n",
      " ['1' 'capit']\n",
      " ['1' 'attempt']\n",
      " ['1' 'capabilities']\n",
      " ['1' 'tablet']\n",
      " ['1' 'build']\n",
      " ['1' 'brows']\n",
      " ['1' 'belief']\n",
      " ['1' 'taiwan']\n",
      " ['1' 'begun']\n",
      " ['1' 'bcommun']\n",
      " ['1' 'award']\n",
      " ['1' 'avail']\n",
      " ['1' 'august']\n",
      " ['1' 'embrac']\n",
      " ['1' 'examin']\n",
      " ['1' 'english']\n",
      " ['1' 'justice']\n",
      " ['1' 'provider']\n",
      " ['1' 'materi']\n",
      " ['1' 'march']\n",
      " ['1' 'maintain']\n",
      " ['1' 'lower']\n",
      " ['1' 'littl']\n",
      " ['1' 'users']\n",
      " ['1' 'question2answ']\n",
      " ['1' 'labor']\n",
      " ['1' 'korea']\n",
      " ['1' 'wherein']\n",
      " ['1' 'japanes']\n",
      " ['1' 'environ']\n",
      " ['1' 'japan']\n",
      " ['1' 'raban']\n",
      " ['1' 'reason']\n",
      " ['1' 'interperson']\n",
      " ['1' 'internet']\n",
      " ['1' 'interfac']\n",
      " ['1' 'interests']\n",
      " ['1' 'recognit']\n",
      " ['1' 'inspir']\n",
      " ['1' 'insid']\n",
      " ['1' 'inline']\n",
      " ['1' 'mention']\n",
      " ['1' 'mislead']\n",
      " ['1' 'process']\n",
      " ['1' 'moderation']\n",
      " ['1' 'participation']\n",
      " ['1' 'parallel']\n",
      " ['1' 'ownership']\n",
      " ['1' 'overflow']\n",
      " ['1' 'outsourc']\n",
      " ['1' 'platforms']\n",
      " ['1' 'origin']\n",
      " ['1' 'organization']\n",
      " ['1' 'organ']\n",
      " ['1' 'popular']\n",
      " ['1' 'often']\n",
      " ['1' 'offer']\n",
      " ['1' 'posit']\n",
      " ['1' 'numer']\n",
      " ['1' 'power']\n",
      " ['1' 'notic']\n",
      " ['1' 'newer']\n",
      " ['1' 'predecessor']\n",
      " ['1' 'negative']\n",
      " ['1' 'problem']\n",
      " ['1' 'mutual']\n",
      " ['1' 'motivations']\n",
      " ['1' 'website']\n",
      " ['1' 'information']\n",
      " ['1' 'wonder']\n",
      " ['1' 'industries']\n",
      " ['1' 'frequent']\n",
      " ['1' 'format']\n",
      " ['1' 'follow']\n",
      " ['1' 'financi']\n",
      " ['1' 'respect']\n",
      " ['1' 'features']\n",
      " ['1' 'respond']\n",
      " ['1' 'faster']\n",
      " ['1' 'screen']\n",
      " ['1' 'search']\n",
      " ['1' 'express']\n",
      " ['1' 'expos']\n",
      " ['1' 'explicit']\n",
      " ['1' 'septemb']\n",
      " ['1' 'experts']\n",
      " ['1' 'expertis']\n",
      " ['1' 'undefin']\n",
      " ['1' 'sever']\n",
      " ['1' 'excel']\n",
      " ['1' 'shift']\n",
      " ['1' 'particularli']\n",
      " ['1' 'everyday']\n",
      " ['1' 'evalu']\n",
      " ['1' 'establish']\n",
      " ['1' 'foundat']\n",
      " ['1' 'fulfil']\n",
      " ['1' 'industri']\n",
      " ['1' 'fulli']\n",
      " ['1' 'recruit']\n",
      " ['1' 'refer']\n",
      " ['1' 'indic']\n",
      " ['1' 'include']\n",
      " ['1' 'registration']\n",
      " ['1' 'incent']\n",
      " ['1' 'improv']\n",
      " ['1' 'import']\n",
      " ['1' 'implement']\n",
      " ['1' 'regularli']\n",
      " ['1' 'reliabl']\n",
      " ['1' 'highli']\n",
      " ['1' 'highest']\n",
      " ['1' 'heterogen']\n",
      " ['1' 'hesit']\n",
      " ['1' 'harper']\n",
      " ['1' 'repres']\n",
      " ['1' 'given']\n",
      " ['1' 'generally']\n",
      " ['1' 'reputation']\n",
      " ['1' 'gather']\n",
      " ['1' 'further']\n",
      " ['1' 'function']\n",
      " ['1' 'above']]\n",
      "Hello, I am a chatbot and I can talk about the following topics :  ['qampa', 'answer', 'question', 'commun', 'onlin', 'peopl', 'softwar', 'motiv', 'servic', 'contribut']\n"
     ]
    }
   ],
   "source": [
    "# Get the theme of the knowledge base\n",
    "word_tokens2 = preprocessing(word_tok)\n",
    "\n",
    "list_to_remove = ['b', \"their\", \"based\", \"which\", 'would', 'https']\n",
    "wc, keywords, mat_sort = get_word_count_uniquewords(word_tokens2, list_to_remove)\n",
    "\n",
    "print(\"Hello, I am a chatbot and I can talk about the following topics : \", keywords[0:10])\n",
    "#print(\"Top 10 wc : \", wc[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6d5719aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a question to the chatbot: when was qampa first made\n",
      "when was qampa first made\n"
     ]
    }
   ],
   "source": [
    "# Get user input about the topics it can talk about\n",
    "inp = input(\"Type a question to the chatbot: \")\n",
    " \n",
    "# prints inp\n",
    "print(inp)\n",
    "inp = 'when were chatbots first made'\n",
    "inp = 'when did chatbots become popular'\n",
    "inp = 'when was qampa first made'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "08b9b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_wt0:  ['when', 'was', 'qampa', 'first', 'made']\n",
      "inp_wt:  ['when', 'wa', 'qampa', 'first', 'made']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['when']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine which sentences are relate to the main themes and {who, what, when, where, why}\n",
    "\n",
    "# who = name, he, she, Mr, Mrs, Ms\n",
    "# what = keywords\n",
    "# when = number!  ---> search for months, dates, time, year, when was\n",
    "# where = location\n",
    "# why = because, due to the fact\n",
    "\n",
    "# verbs ?? = action\n",
    "\n",
    "# word token input\n",
    "inp_wt0 = inp.split()\n",
    "print('inp_wt0: ', inp_wt0)\n",
    "\n",
    "# Stem the input text\n",
    "ps = PorterStemmer()\n",
    "inp_wt = []\n",
    "for w in inp_wt0:\n",
    "    inp_wt.append(ps.stem(w))\n",
    "print('inp_wt: ', inp_wt)\n",
    "\n",
    "# Simple framework for setting up a Q&A transformer model: Retrival of key sentences in the knowledge data source\n",
    "q_type = []\n",
    "for i in inp_wt:\n",
    "    if i in ['who', 'name of']:\n",
    "        q_type.append('who')\n",
    "        # Need a function that detects names\n",
    "        list_to_search = ['he', 'she', 'they', 'Mr', 'Mrs', 'Ms']\n",
    "        # Search knowledge for sentences that contain names, pronouns (he, she, they, Mr, Mrs, Ms)\n",
    "    elif i in ['what']:\n",
    "        q_type.append('what')\n",
    "        # Search knowledge for sentences that contain knowledge keywords similar to user keywords\n",
    "    elif i in ['when']:\n",
    "        q_type.append('when')  # ONLY DOING 'when' for the moment\n",
    "    elif i in ['where']:\n",
    "        q_type.append('where')\n",
    "    elif i in ['why']:\n",
    "        q_type.append('why')\n",
    "q_type        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "81747cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sen_with_nums:  [9, 11, 14, 16, 17, 21, 23, 24, 41, 63, 65, 69]\n",
      "sen_with_timetext:  [11, 14, 16, 17, 18, 21, 23, 36]\n",
      "relv_sen_unique:  [ 9 11 14 16 17 18 21 23 24 36 41 63 65 69]\n"
     ]
    }
   ],
   "source": [
    "# Based on the Question Type: find the relavent sentences in the Knowledge Source that could Answer the Question Type\n",
    "for i in q_type:\n",
    "    \n",
    "    if i == 'when':\n",
    "        # -----------------------------------------\n",
    "        # What to do if \"when\" is in the input:\n",
    "        # -----------------------------------------\n",
    "        out = []\n",
    "        for i in word_tok:\n",
    "            # Search to see if there are '.' at the end of the word\n",
    "            i = remove_chars_from_wordtoken(i, '.', '')\n",
    "            i = remove_chars_from_wordtoken(i, '%', '')\n",
    "            i = remove_chars_from_wordtoken(i, '$', '')\n",
    "\n",
    "            # First determine if a number is a ratio\n",
    "            i = remove_chars_from_wordtoken(i, '/', ' divided by ')\n",
    "\n",
    "            # Second determine if a number is present\n",
    "            if i.upper() == i.lower() and i != '':\n",
    "                # is a number\n",
    "                out.append(int(i))\n",
    "\n",
    "        imp_nums = np.unique(out)\n",
    "        imp_nums_str = [str(i) for i in imp_nums]\n",
    "        \n",
    "        # Search knowledge for sentences that contain numbers and text referring to time \n",
    "        # -----------------------------------------\n",
    "        # 1) Get the sentences that contain numbers\n",
    "        sen_with_nums = []\n",
    "        for ind, i in enumerate(sen):\n",
    "            # Returns the unique values in 1st array NOT in 2nd array\n",
    "            vals_from_long_not_in_short = np.setdiff1d(imp_nums_str, i)\n",
    "            out = np.setdiff1d(imp_nums_str, vals_from_long_not_in_short)\n",
    "            if any(out):\n",
    "                sen_with_nums.append(ind)\n",
    "        # -----------------------------------------\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # 2) Get the sentences \n",
    "        list_to_search = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December', 'year', 'month', 'week', 'day', 'hour', 'minute', 'second', 'ms', 'millisecond']\n",
    "        sen_with_timetext = []\n",
    "        for ind, i in enumerate(sen):\n",
    "            # Returns the unique values in 1st array NOT in 2nd array\n",
    "            vals_from_long_not_in_short = np.setdiff1d(list_to_search, i)\n",
    "            out = np.setdiff1d(list_to_search, vals_from_long_not_in_short)\n",
    "            if any(out):\n",
    "                sen_with_timetext.append(ind)\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        \n",
    "print('sen_with_nums: ', sen_with_nums)\n",
    "print('sen_with_timetext: ', sen_with_timetext)\n",
    "\n",
    "# Combine 'relevant sentence' lists\n",
    "sen_with_nums += sen_with_timetext\n",
    "relv_sen_unique = np.unique(sen_with_nums)\n",
    "print('relv_sen_unique: ', relv_sen_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "13b42241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamilah\\AppData\\Local\\Temp/ipykernel_9672/2203938722.py:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{21: [20.0, 0.4472135954999579, 18],\n",
       " 9: [20.0, 0.4472135954999579, 16],\n",
       " 11: [0.0, nan, 22],\n",
       " 14: [0.0, nan, 18],\n",
       " 16: [0.0, nan, 10],\n",
       " 17: [0.0, nan, 26],\n",
       " 18: [0.0, nan, 6],\n",
       " 23: [0.0, nan, 18],\n",
       " 24: [0.0, nan, 24],\n",
       " 36: [0.0, nan, 20],\n",
       " 41: [0.0, nan, 33],\n",
       " 63: [0.0, nan, 43],\n",
       " 65: [0.0, nan, 16],\n",
       " 69: [0.0, nan, 24]}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for common words in the input with each of 'relevant sentences'\n",
    "cos_sim_all = get_cossine_similarity(inp_wt, relv_sen_unique)\n",
    "cos_sim_all = make_a_properlist(cos_sim_all)\n",
    "\n",
    "# 1) Get a Percentage of how many words of 'input' are used in sen\n",
    "dict_val = {}\n",
    "for ind, rels in enumerate(relv_sen_unique):\n",
    "    tot = np.zeros((len(inp_wt),1))\n",
    "    for i in range(len(inp_wt)):\n",
    "        for j in range(len(sen[rels])):\n",
    "            if inp_wt[i] == sen[rels][j]:\n",
    "                tot[i] = 1\n",
    "                break\n",
    "\n",
    "    per = sum(np.ravel(tot))/len(inp_wt) * 100\n",
    "    \n",
    "    dict_val[rels] = [per, cos_sim_all[ind], len(sen[rels])]  # Percentage and cos-sine similarity\n",
    "\n",
    "# Sort by max\n",
    "dict_val_sorted = sort_dict_by_value(dict_val, reverse = True)\n",
    "dict_val_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ab77d9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_val ['2009' '21']\n",
      "chatbot_output :  ['b', 'was', 'founded', 'in', 'June', '2009', 'while', 'the', 'website', 'was', 'made', 'available', 'to', 'the', 'public', 'on', 'June', '21']\n"
     ]
    }
   ],
   "source": [
    "# So the Longest sentence WITH the Highest similarity appears to be the best response!!\n",
    "chatbot_output = sen[list(dict_val_sorted.keys())[0]]\n",
    "\n",
    "\n",
    "vals_from_long_not_in_short = np.setdiff1d(imp_nums_str, chatbot_output)\n",
    "out = np.setdiff1d(imp_nums_str, vals_from_long_not_in_short)\n",
    "if any(out):\n",
    "    print('number_val', out)\n",
    "\n",
    "print('chatbot_output : ', chatbot_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1940a",
   "metadata": {},
   "source": [
    "I asked some simple questions, so I think I got Luckly!\n",
    "\n",
    "I think it could be improved with a Q&A transformer step with the 'relavant sentences'!  And, I was a little lazy about separating the sentences perfectly using the webscraped words.  If the sentences were perfectly separated, it would be interesting to see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ffcff216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
